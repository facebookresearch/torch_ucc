name: CI

on: [push, pull_request]

env:
  OPENUCX_LINK: https://github.com/openucx/ucx.git
  UCC_LINK: https://github.com/openucx/ucc.git
  PARAM_LINK: https://github.com/facebookresearch/param

jobs:
  tests:
    runs-on: ubuntu-latest
    container:
      image: pytorch/pytorch:latest
    steps:
    - name: Install packages
      run: |
        apt-get update
        apt-get install -y --no-install-recommends build-essential git cmake libtool-bin wget autoconf automake
        conda uninstall -y pytorch torchvision
        pip3 install --pre torch -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html
    - name: Get UCX
      run: |
        git clone ${OPENUCX_LINK} /tmp/ucx
        cd /tmp/ucx
        ./autogen.sh
        ./contrib/configure-release-mt --without-java --disable-numa --prefix=/opt/ucx
        make -j install
    - name: Get UCC
      run: |
        git clone ${UCC_LINK} /tmp/ucc
        cd /tmp/ucc
        ./autogen.sh
        ./configure --with-ucx=/opt/ucx --prefix=/opt/ucc
        make -j install
    - uses: actions/checkout@v1
    - name: Build with UCX and UCC
      run: |
        UCX_HOME=/opt/ucx/ UCC_HOME=/opt/ucc/ WITH_CUDA=no python setup.py install
    - name: Tests
      run: |
        export LD_LIBRARY_PATH=/opt/ucx/lib:/opt/ucc/lib:$LD_LIBRARY_PATH
        /opt/ucx/bin/ucx_info -e -u t
        export UCX_LOG_LEVEL=info
        for np in `seq 4`
        do
          echo "Test comm size $np"
          export TORCH_UCC_TEST_SIZE=$np
          echo "UCC barrier"
          /bin/bash ./test/start_test.sh ./test/torch_barrier_test.py --backend=gloo
          echo "UCC alltoall"
          /bin/bash ./test/start_test.sh ./test/torch_alltoall_test.py --backend=gloo
          echo "UCC alltoallv"
          /bin/bash ./test/start_test.sh ./test/torch_alltoallv_test.py --backend=gloo
          echo "UCC allgather"
          /bin/bash ./test/start_test.sh ./test/torch_allgather_test.py --backend=gloo
          echo "UCC allreduce"
          /bin/bash ./test/start_test.sh ./test/torch_allreduce_test.py --backend=gloo
          echo "UCC broadcast"
          /bin/bash ./test/start_test.sh ./test/torch_bcast_test.py --backend=gloo
        done
        echo "UCC basic functionality test"
        /bin/bash ./test/start_test.sh ./test/torch_work_test.py --backend=gloo
        echo "UCC pt2pt"
        /bin/bash ./test/start_test.sh ./test/torch_pt2pt_test.py --backend=gloo
        echo "UCC timeout test"
        /bin/bash ./test/start_test.sh ./test/torch_timeout_test.py --backend=gloo
    - name: Test PARAM
      run: |
        git clone ${PARAM_LINK} /tmp/param
        export LD_LIBRARY_PATH=/opt/ucx/lib:/opt/ucc/lib:$LD_LIBRARY_PATH
        export TORCH_UCC_TEST_SIZE=4
        echo "PARAM-Comms Allreduce w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --collective all_reduce
        echo "PARAM-Comms Alltoall w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --collective all_to_all
        echo "PARAM-Comms Alltoallv w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --collective all_to_allv
        echo "PARAM-Comms Broadcast w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --collective broadcast
        echo "PARAM-Comms Allgather w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --collective all_gather
        echo "PARAM-Comms Quantized Allreduce w/ UCC (use of c10d future)"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --bitwidth 16 --collective all_reduce
        echo "PARAM-Comms Non-blocking Allreduce w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --z 0 --collective all_reduce
        echo "PARAM-Comms Non-blocking Alltoall w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --z 0 --collective all_to_all
        echo "PARAM-Comms Non-blocking Alltoallv w/ UCC"
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --z 0 --collective all_to_allv
        echo "PARAM-Comms Pt2pt w/ UCC"
        export TORCH_UCC_TEST_SIZE=2
        /bin/bash ./test/start_test.sh /tmp/param/train/comms/pt/comms.py --backend ucc --device cpu --b 4 --e 4M --c 1 --pt2pt one2one --src-ranks 0 --dst-ranks 1
    - name: PyTorch distributed tests
      run: |
        set -e
        git_version=$(python -c 'import torch; print(torch.version.git_version)')
        torch_ucc_file=$(python -c "import inspect; import torch; import torch_ucc; print(inspect.getfile(torch_ucc))")
        python -c "import torch.distributed as dist; assert dist.is_nccl_available()"
        pip3 install expecttest
        git clone https://github.com/pytorch/pytorch.git
        cd pytorch
        git checkout ${git_version}
        python test/run_test.py --verbose --nccl -i distributed/test_distributed_spawn
